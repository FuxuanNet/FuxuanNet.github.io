# 线性回归的原理
## 房价预测

- 假设 1：影响房价的关键因素是卧室个数，卫生间个数和居住面积，记为 $x_1$,$x_2$,$x_3$.
- 假设 2：成交价是关键因素的加权和
$$
y=w_1x_1+w_2x_2+w_3x_3+b
$$
## 线性模型
- 给定 n 维输入

$$\mathbf{x}=[x_1,x_2,…,x_n]^T$$

- 线性模型有一个 n 维权重和一个标量偏差

$$\mathbf{w}=[w_1,w_2,…,w_n]^T,\,b$$

- 输出是输入的加权和

$$y=w_1x_1+w_2x_2+…+w_nx_n+b$$

- 向量版本：
$$y=<\mathbf{w},\mathbf{x}>+b$$

- 可以看成单层神经网络

## 衡量预估质量

- 比较真实值和预估值，例如房屋售价和估价
- 假设 $y$ 是真实值，$\hat{y}$ 是预估值，我们可以比较**平方损失**：
$$
l(y,\hat{y})=\frac{1}{2}(y- \hat{y})^2
$$

## 训练数据

- 采集权重和偏差
- 假设有 n 个样本，记

$$\mathbf{X}=[x_1,x_2,…,x_n]^T \,\,\,\mathbf{y}=[y_1,y_2,…,y_n]^T$$

- 训练损失：找到 $w_1$ 使得这一项最小
$$
l(\mathbf{X},\mathbf{y},\mathbf{w},b)=\frac{1}{2n}||\mathbf{y}-\mathbf{Xw}-b||^2
$$
- 最小化损失来学习参数
$$\mathbf{w}^*,\mathbf{b}^*=arg\, {min}_{w,b}l(\mathbf{X},\mathbf{y},\mathbf{w},b)$$
## 显示解


# 基础优化方法 
## 梯度下降 
- 挑选一个初始值 $w_0$
- 重复迭代参数 $t=1,2,3$
$$\mathbf{W}_t= \mathbf{W_{t-1}-\eta \frac{\delta l}{\delta W_{t-1}}}$$
- 沿梯度方向将增加损失函数值
- 学习率：步长的超参数
## 小批量随机梯度下降
- 随机采样 b 个样本 $i_1, i_2,…,i_b$ 来近似损失 
$$\frac{1}{b}\sum_{i \in I_b}l(\mathbf{x}_i, y_i, \mathbf{w})$$
- b 是批量大小，另一个重要的超参数
## 总结
- 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降时深度学习默认的求解算法
- 两个重要的超参数是批量大小和学习率
# 线性回归的从零开始实现

## 构造人造数据集
$$
\mathbf{w}=[2,-3.4]^T,\,b=4.2,\,\text{和噪声项 } \delta
$$

```python
def synthetic_data(w, b,  num_examples):
	X = torch.normal(0, 1, (num_examples, len(w)))
	y = torch.matmul(X, w) + b
	y += torch.normal(0, 0.01, y.shape)
	return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

## 定义 `data_iter` 函数
```python

```