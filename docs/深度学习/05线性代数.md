---
title: 动手学深度学习之 05 线性代数
date: 2024-01-31
details: 李沐老师的动手学深度学习的第五讲： 线性代数
showInHome: false
---

# 动手学深度学习之线性代数

## 数学知识

> 补充一些快要忘记的数学知识

## 范数

$$
c = A \cdot b\,\, \text{hence}\,\, ||c|| \leq ||A|| \cdot ||b||
$$

- 取决于如何衡量 b 和 c 的长度

**常见的范数**：

- 矩阵范数：最小的满足上面公式的值

- Frobenius范数：$\sqrt{\sum_{i,j} A_{ij}^2}$

## 正定

一个矩阵乘任何的一个列向量或是行向量都大于等于零。

$$
||x||^2 = x^T x \geq 0 \,\, \text{generalizes to}\,\, x^T A x \geq 0
$$

## 正交矩阵

- 所有行都正交

- 所有行都有单位长度

$$
U\,\, \text{with}\,\, \sum_j U_{ij} U_{ij} = \delta_{jk}
$$

- 可以写成

$$UU^T=1$$

## 置换矩阵

$$
P \,\, \text{with}\,\, P_{ij} = 1 \,\, \text{if and only if}\,\, j = \pi(i)
$$

- 置换矩阵都是正交矩阵

## 特征向量

- 不被矩阵改变的向量

$$
Ax = \lambda x
$$

- 对称矩阵总是可以找到特征向量

## 编程计算

## 标量

由只有一个元素的张量表示。

```python
x = torch.tensor([3.0])
```

## 向量

由向量值组成的列表。

```python
x = torch.arange(4)
```

## 矩阵

```python
A = torch.arange(20).reshape(5, 4)
```

转置：`A.T`

## 求和

部分内容见上一讲 [04数据处理](./04数据操作处理.md)。

对矩阵而言，0 是行，对 `axis=0` 求和就是对每一列求和，因为要把矩阵的第 0 维降维，也就是将原本的行数降维，只保留原本的列数。等价于去掉形状的对应维度。

## 哈达玛积

$A \odot B$ 代表按元素相乘：$A_{ij} \cdot B_{ij}$。

```python
A * B
```

## 向量的点积

点积是相同位置的按元素乘积的和。

> - 向量的点积结果是标量。

``` python
torch.dot(x, y)
```

也可以先执行按元素乘法，然后进行求和来表示两个向量的点积。

```python
torch.sum(x * y)
```

## 矩阵-向量积

- 矩阵-向量积是矩阵的每一行与向量的点积。

```python
y = torch.mv(A, x)
```

## 矩阵-矩阵乘法

- 矩阵-矩阵乘法 AB 看作是简单地执行 m 次矩阵-向量积，并将结果拼接在一起，形成一个 $n \times m$ 矩阵。

```python
A = torch.arange(20).reshape(5, 4)
B = torch.ones(4, 3)
torch.mm(A, B)
```

## 范数求解

- $L_2$ 范数：

$$
||x||_2 = \sqrt{\sum_i^n x_i^2}
$$

``` python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```

- $L_1$ 范数：

$$
||x||_1 = \sum_i^n |x_i|
$$

```python
torch.abs(u).sum()
```

- 矩阵的 Frobenius 范数：

$$
||X||_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n X_{ij}^2}
$$

```python
torch.norm(torch.ones((4, 9)))
```
